n_blocks: 24
residual_dropout: 0.0
ffn_d_hidden: 1365
d_token: 1024
attention_dropout: 0.3
ffn_dropout: 0.3
attention_n_heads: 16
attention_initialization: kaiming
ffn_activation: ReGLU
attention_normalization: LayerNorm
ffn_normalization: LayerNorm
prenormalization: True
first_prenormalization: False
last_layer_query_idx: null
n_tokens: null
kv_compression_ratio: null
kv_compression_sharing: null
